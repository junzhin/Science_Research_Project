Namespace(arch='resnet50', batch_size=32, center_crop=False, data='Office31', data_processing='ours', epochs=10, iters_per_epoch=500, log='logs/src_only/Office31_A2W', lr=0.001, lr_decay=0.75, lr_gamma=0.0003, momentum=0.9, per_class_eval=False, phase='train', print_freq=100, root='/home/junzhin/Project/Summer_project/code/version2.0/data/office31', seed=0, source='A', target='W', wd=0.0005, workers=2)
source_only.py:41: UserWarning: You have chosen to seed training. This will turn on the CUDNN deterministic setting, which can slow down your training considerably! You may see unexpected behavior when restarting from checkpoints.
  warnings.warn('You have chosen to seed training. '
=> using pre-trained model 'resnet50'
Epoch: [0][  0/500]	Time 0.300 (0.300)	Data 0.015 (0.015)	Loss 3.39 (3.39)	Cls Acc 3.1 (3.1)
Epoch: [0][100/500]	Time 0.227 (0.230)	Data 0.082 (0.083)	Loss 1.78 (2.67)	Cls Acc 78.1 (42.5)
Epoch: [0][200/500]	Time 0.226 (0.229)	Data 0.080 (0.083)	Loss 1.17 (2.06)	Cls Acc 81.2 (59.0)
Epoch: [0][300/500]	Time 0.226 (0.229)	Data 0.080 (0.083)	Loss 0.73 (1.71)	Cls Acc 81.2 (66.2)
Epoch: [0][400/500]	Time 0.226 (0.229)	Data 0.080 (0.083)	Loss 0.82 (1.48)	Cls Acc 84.4 (70.3)
Test: [ 0/25]	Time  0.904 ( 0.904)	Loss 1.1218e+00 (1.1218e+00)	Acc@1  68.75 ( 68.75)	Acc@5 100.00 (100.00)
 * Acc@1 75.346 Acc@5 94.465
acc1 = 75.3, best_acc1 = 75.3
Epoch: [1][  0/500]	Time 0.160 (0.160)	Data 0.015 (0.015)	Loss 0.81 (0.81)	Cls Acc 84.4 (84.4)
Epoch: [1][100/500]	Time 0.226 (0.228)	Data 0.080 (0.082)	Loss 0.74 (0.61)	Cls Acc 75.0 (85.3)
Epoch: [1][200/500]	Time 0.226 (0.228)	Data 0.081 (0.083)	Loss 0.37 (0.58)	Cls Acc 93.8 (85.9)
Epoch: [1][300/500]	Time 0.226 (0.229)	Data 0.081 (0.084)	Loss 0.38 (0.55)	Cls Acc 90.6 (86.5)
Epoch: [1][400/500]	Time 0.226 (0.229)	Data 0.080 (0.084)	Loss 0.32 (0.53)	Cls Acc 93.8 (87.1)
Test: [ 0/25]	Time  0.399 ( 0.399)	Loss 1.2539e+00 (1.2539e+00)	Acc@1  43.75 ( 43.75)	Acc@5 100.00 (100.00)
 * Acc@1 77.233 Acc@5 94.591
acc1 = 77.2, best_acc1 = 77.2
Epoch: [2][  0/500]	Time 0.159 (0.159)	Data 0.014 (0.014)	Loss 0.29 (0.29)	Cls Acc 93.8 (93.8)
Epoch: [2][100/500]	Time 0.225 (0.228)	Data 0.080 (0.083)	Loss 0.54 (0.40)	Cls Acc 87.5 (89.9)
Epoch: [2][200/500]	Time 0.226 (0.228)	Data 0.081 (0.083)	Loss 0.50 (0.39)	Cls Acc 87.5 (90.6)
Epoch: [2][300/500]	Time 0.226 (0.229)	Data 0.080 (0.083)	Loss 0.41 (0.38)	Cls Acc 93.8 (91.1)
Epoch: [2][400/500]	Time 0.226 (0.228)	Data 0.081 (0.083)	Loss 0.55 (0.37)	Cls Acc 84.4 (91.3)
Test: [ 0/25]	Time  0.403 ( 0.403)	Loss 1.0909e+00 (1.0909e+00)	Acc@1  43.75 ( 43.75)	Acc@5 100.00 (100.00)
 * Acc@1 76.730 Acc@5 95.220
acc1 = 76.7, best_acc1 = 77.2
Epoch: [3][  0/500]	Time 0.158 (0.158)	Data 0.013 (0.013)	Loss 0.20 (0.20)	Cls Acc 93.8 (93.8)
Epoch: [3][100/500]	Time 0.226 (0.228)	Data 0.080 (0.082)	Loss 0.47 (0.29)	Cls Acc 87.5 (93.4)
Epoch: [3][200/500]	Time 0.226 (0.228)	Data 0.080 (0.083)	Loss 0.15 (0.29)	Cls Acc 96.9 (93.5)
Epoch: [3][300/500]	Time 0.226 (0.228)	Data 0.081 (0.083)	Loss 0.34 (0.28)	Cls Acc 90.6 (93.7)
Epoch: [3][400/500]	Time 0.226 (0.228)	Data 0.081 (0.083)	Loss 0.19 (0.28)	Cls Acc 96.9 (93.8)
Traceback (most recent call last):
  File "source_only.py", line 325, in <module>
    
  File "source_only.py", line 141, in main
    lr_scheduler, epoch, args)
  File "source_only.py", line 198, in train
    losses.update(loss.item(), x_s.size(0))
KeyboardInterrupt
