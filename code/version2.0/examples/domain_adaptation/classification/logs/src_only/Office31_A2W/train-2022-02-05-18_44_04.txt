Namespace(arch='alexnet', batch_size=32, center_crop=False, checkmodel_logsave=False, data='Office31', data_processing='ours', epochs=50, iters_per_epoch=500, local=True, local_pretrained_path='/home/junzhin/Project/Summer_project/code/version1.0/pretrained_model_results/ALEX_office31masked2Random_newmodel_best.pth.tar', log='logs/src_only/Office31_A2W', lr=0.001, lr_decay=0.75, lr_gamma=0.0003, momentum=0.9, per_class_eval=False, phase='train', pretrained=True, print_freq=100, root='/home/junzhin/Project/Summer_project/code/version2.0/data/office31', seed=None, source='A', target='W', wd=0.0005, workers=2)
=> using pre-trained model 'alexnet'
Loading the local alexnet pretrained model weights!
Epoch: [0][  0/500]	Time 0.382 (0.382)	Data 0.013 (0.013)	Loss 3.44 (3.44)	Cls Acc 0.0 (0.0)
Epoch: [0][100/500]	Time 0.119 (0.077)	Data 0.112 (0.067)	Loss 3.43 (3.43)	Cls Acc 9.4 (3.7)
Epoch: [0][200/500]	Time 0.120 (0.076)	Data 0.113 (0.067)	Loss 3.44 (3.43)	Cls Acc 0.0 (3.7)
Epoch: [0][300/500]	Time 0.125 (0.075)	Data 0.118 (0.067)	Loss 3.45 (3.43)	Cls Acc 0.0 (3.6)
Epoch: [0][400/500]	Time 0.118 (0.075)	Data 0.111 (0.067)	Loss 3.42 (3.43)	Cls Acc 3.1 (3.5)
Test: [ 0/25]	Time  0.368 ( 0.368)	Loss 3.4264e+00 (3.4264e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 3.145 Acc@5 20.881
acc1 = 3.1, best_acc1 = 3.1
Epoch: [1][  0/500]	Time 0.020 (0.020)	Data 0.013 (0.013)	Loss 3.42 (3.42)	Cls Acc 6.2 (6.2)
Epoch: [1][100/500]	Time 0.116 (0.072)	Data 0.109 (0.065)	Loss 3.43 (3.42)	Cls Acc 3.1 (3.8)
Epoch: [1][200/500]	Time 0.120 (0.073)	Data 0.113 (0.066)	Loss 3.41 (3.42)	Cls Acc 6.2 (3.6)
Epoch: [1][300/500]	Time 0.118 (0.074)	Data 0.111 (0.067)	Loss 3.42 (3.42)	Cls Acc 0.0 (3.7)
Epoch: [1][400/500]	Time 0.117 (0.074)	Data 0.110 (0.067)	Loss 3.42 (3.42)	Cls Acc 6.2 (3.6)
Test: [ 0/25]	Time  0.342 ( 0.342)	Loss 3.4205e+00 (3.4205e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 3.774 Acc@5 21.132
acc1 = 3.8, best_acc1 = 3.8
Epoch: [2][  0/500]	Time 0.022 (0.022)	Data 0.014 (0.014)	Loss 3.43 (3.43)	Cls Acc 3.1 (3.1)
Epoch: [2][100/500]	Time 0.118 (0.072)	Data 0.111 (0.065)	Loss 3.42 (3.42)	Cls Acc 9.4 (3.9)
Epoch: [2][200/500]	Time 0.123 (0.073)	Data 0.116 (0.066)	Loss 3.41 (3.41)	Cls Acc 0.0 (4.1)
Epoch: [2][300/500]	Time 0.115 (0.073)	Data 0.108 (0.066)	Loss 3.40 (3.41)	Cls Acc 3.1 (4.4)
Epoch: [2][400/500]	Time 0.115 (0.073)	Data 0.108 (0.066)	Loss 3.45 (3.41)	Cls Acc 0.0 (4.4)
Test: [ 0/25]	Time  0.356 ( 0.356)	Loss 3.4076e+00 (3.4076e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 2.642 Acc@5 18.742
acc1 = 2.6, best_acc1 = 3.8
Epoch: [3][  0/500]	Time 0.021 (0.021)	Data 0.014 (0.014)	Loss 3.44 (3.44)	Cls Acc 0.0 (0.0)
Epoch: [3][100/500]	Time 0.120 (0.073)	Data 0.113 (0.066)	Loss 3.45 (3.40)	Cls Acc 0.0 (4.7)
Epoch: [3][200/500]	Time 0.117 (0.074)	Data 0.110 (0.067)	Loss 3.36 (3.40)	Cls Acc 0.0 (5.2)
Epoch: [3][300/500]	Time 0.119 (0.074)	Data 0.112 (0.067)	Loss 3.38 (3.40)	Cls Acc 3.1 (5.3)
Epoch: [3][400/500]	Time 0.114 (0.074)	Data 0.107 (0.067)	Loss 3.35 (3.40)	Cls Acc 12.5 (5.4)
Test: [ 0/25]	Time  0.356 ( 0.356)	Loss 3.4084e+00 (3.4084e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 2.893 Acc@5 22.138
acc1 = 2.9, best_acc1 = 3.8
Epoch: [4][  0/500]	Time 0.021 (0.021)	Data 0.013 (0.013)	Loss 3.36 (3.36)	Cls Acc 3.1 (3.1)
Epoch: [4][100/500]	Time 0.118 (0.072)	Data 0.111 (0.065)	Loss 3.35 (3.37)	Cls Acc 9.4 (6.4)
Epoch: [4][200/500]	Time 0.351 (0.074)	Data 0.343 (0.067)	Loss 3.29 (3.37)	Cls Acc 9.4 (6.6)
Epoch: [4][300/500]	Time 0.120 (0.074)	Data 0.113 (0.067)	Loss 3.42 (3.37)	Cls Acc 6.2 (7.1)
Epoch: [4][400/500]	Time 0.121 (0.074)	Data 0.114 (0.067)	Loss 3.32 (3.36)	Cls Acc 3.1 (7.7)
Test: [ 0/25]	Time  0.361 ( 0.361)	Loss 3.4099e+00 (3.4099e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 6.289 Acc@5 23.145
acc1 = 6.3, best_acc1 = 6.3
Epoch: [5][  0/500]	Time 0.022 (0.022)	Data 0.015 (0.015)	Loss 3.29 (3.29)	Cls Acc 12.5 (12.5)
Epoch: [5][100/500]	Time 0.117 (0.073)	Data 0.110 (0.066)	Loss 3.23 (3.32)	Cls Acc 9.4 (9.0)
Epoch: [5][200/500]	Time 0.117 (0.074)	Data 0.110 (0.067)	Loss 3.40 (3.31)	Cls Acc 6.2 (9.1)
Epoch: [5][300/500]	Time 0.118 (0.075)	Data 0.111 (0.067)	Loss 3.21 (3.30)	Cls Acc 12.5 (9.2)
Epoch: [5][400/500]	Time 0.120 (0.075)	Data 0.113 (0.068)	Loss 3.30 (3.30)	Cls Acc 9.4 (9.3)
Test: [ 0/25]	Time  0.349 ( 0.349)	Loss 3.3650e+00 (3.3650e+00)	Acc@1   0.00 (  0.00)	Acc@5   0.00 (  0.00)
 * Acc@1 6.667 Acc@5 22.013
acc1 = 6.7, best_acc1 = 6.7
Epoch: [6][  0/500]	Time 0.021 (0.021)	Data 0.014 (0.014)	Loss 3.47 (3.47)	Cls Acc 6.2 (6.2)
Epoch: [6][100/500]	Time 0.137 (0.074)	Data 0.130 (0.067)	Loss 3.35 (3.27)	Cls Acc 6.2 (10.4)
Epoch: [6][200/500]	Time 0.118 (0.075)	Data 0.111 (0.068)	Loss 3.20 (3.27)	Cls Acc 15.6 (10.3)
Epoch: [6][300/500]	Time 0.106 (0.075)	Data 0.099 (0.068)	Loss 3.24 (3.27)	Cls Acc 3.1 (10.3)
Epoch: [6][400/500]	Time 0.120 (0.075)	Data 0.113 (0.068)	Loss 3.18 (3.26)	Cls Acc 9.4 (10.5)
Traceback (most recent call last):
  File "source_only.py", line 333, in <module>
    main(args)
  File "source_only.py", line 141, in main
    lr_scheduler, epoch, args)
  File "source_only.py", line 204, in train
    optimizer.step()
  File "/home/junzhin/anaconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 65, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/junzhin/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    return func(*args, **kwargs)
  File "/home/junzhin/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return func(*args, **kwargs)
  File "/home/junzhin/anaconda3/lib/python3.7/site-packages/torch/optim/sgd.py", line 143, in step
    nesterov=nesterov)
  File "/home/junzhin/anaconda3/lib/python3.7/site-packages/torch/optim/_functional.py", line 176, in sgd
    d_p = d_p.add(buf, alpha=momentum)
KeyboardInterrupt
